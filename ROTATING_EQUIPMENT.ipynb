{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Oynkey2127/ML-SENSOR-FUSION-PROJECT-/blob/main/ROTATING_EQUIPMENT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!pip install -q xgboost imbalanced-learn shap"
      ],
      "metadata": {
        "id": "a6KCN4qTGziS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1db04b6"
      },
      "source": [
        "!pip install -q xgboost imbalanced-learn shap\n",
        "\n",
        "import os, random, joblib, math\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from scipy.stats import skew, kurtosis\n",
        "from scipy.fft import rfft, rfftfreq\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
        "MODEL_DIR = \"/content/models_nocnn\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "DATA_PATH = \"/content/Rotating_equipment_fault_data.csv\"\n",
        "print(\"Loading:\", DATA_PATH)\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"Shape:\", df.shape)\n",
        "display(df.head())\n",
        "\n",
        "\n",
        "# Cell 4: Column detection & cleaning\n",
        "target_col = \"Fault_Type\"\n",
        "sensor_cols = ['Vibration_X','Vibration_Y','Vibration_Z','Acoustic_Level','Temperature']  # from your CSV sample\n",
        "\n",
        "# drop NA rows in required columns\n",
        "df = df.dropna(subset=sensor_cols + [target_col]).reset_index(drop=True)\n",
        "print(\"After dropna:\", df.shape)\n",
        "\n",
        "# label encode target\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df[target_col].values)\n",
        "X_raw = df[sensor_cols].astype(float).values\n",
        "n_classes = len(le.classes_)\n",
        "print(\"Detected classes:\", le.classes_)\n",
        "\n",
        "\n",
        "# Cell 5: Create non-overlapping windows\n",
        "TIMESTEPS = 20  # tune if needed\n",
        "\n",
        "def make_non_overlapping(X, y, timesteps):\n",
        "    Xw, yw = [], []\n",
        "    i = 0\n",
        "    while i + timesteps <= len(X):\n",
        "        Xw.append(X[i:i+timesteps])\n",
        "        yw.append(y[i+timesteps-1])\n",
        "        i += timesteps\n",
        "    return np.array(Xw), np.array(yw)\n",
        "\n",
        "Xw_all, yw_all = make_non_overlapping(X_raw, y, TIMESTEPS)\n",
        "print(\"Windows shape (non-overlap):\", Xw_all.shape, yw_all.shape)\n",
        "\n",
        "\n",
        "# Cell 6: Time-ordered split (train = first 80%, test = last 20%)\n",
        "n = len(Xw_all)\n",
        "split_idx = int(0.8 * n)\n",
        "Xw_train, Xw_test = Xw_all[:split_idx], Xw_all[split_idx:]\n",
        "yw_train, yw_test = yw_all[:split_idx], yw_all[split_idx:]\n",
        "print(\"Train windows:\", Xw_train.shape, \"Test windows:\", Xw_test.shape)\n",
        "\n",
        "\n",
        "# Cell 7: Feature engineering functions\n",
        "def time_feats(window):\n",
        "    feats = []\n",
        "    for ch in range(window.shape[1]):\n",
        "        arr = window[:, ch]\n",
        "        feats += [\n",
        "            arr.mean(),\n",
        "            arr.std(ddof=0),\n",
        "            np.sqrt(np.mean(arr**2)),    # RMS\n",
        "            np.max(arr) - np.min(arr),   # P2P\n",
        "            np.percentile(arr,75)-np.percentile(arr,25),  # IQR\n",
        "            skew(arr),\n",
        "            kurtosis(arr),\n",
        "            (np.max(np.abs(arr)) / (np.mean(np.abs(arr))+1e-9)),  # crest-ish\n",
        "            np.mean(np.diff(arr))\n",
        "        ]\n",
        "    return np.array(feats)\n",
        "\n",
        "def freq_feats(window, n_bands=6):\n",
        "    feats = []\n",
        "    t = window.shape[0]\n",
        "    fft_idx_edges = np.linspace(0, t//2+1, n_bands+1, dtype=int)\n",
        "    for ch in range(window.shape[1]):\n",
        "        sig = window[:, ch]\n",
        "        fft_vals = np.abs(rfft(sig))\n",
        "        energy = (fft_vals**2).sum() + 1e-9\n",
        "        for b in range(n_bands):\n",
        "            v = fft_vals[fft_idx_edges[b]:fft_idx_edges[b+1]]\n",
        "            feats.append((v**2).sum()/energy)\n",
        "    return np.array(feats)\n",
        "\n",
        "def build_feats(Xw):\n",
        "    out = []\n",
        "    for w in Xw:\n",
        "        feat = np.concatenate([time_feats(w), freq_feats(w, n_bands=6)])\n",
        "        out.append(feat)\n",
        "    return np.vstack(out)\n",
        "\n",
        "\n",
        "# Cell 8: Build features & scale (fit scaler on train only)\n",
        "Xf_train = build_feats(Xw_train)\n",
        "Xf_test  = build_feats(Xw_test)\n",
        "print(\"Engineered feature shapes:\", Xf_train.shape, Xf_test.shape)\n",
        "\n",
        "scaler_feats = StandardScaler().fit(Xf_train)\n",
        "Xf_train_s = scaler_feats.transform(Xf_train)\n",
        "Xf_test_s  = scaler_feats.transform(Xf_test)\n",
        "joblib.dump(scaler_feats, os.path.join(MODEL_DIR, \"scaler_features.pkl\"))\n",
        "print(\"Saved scaler to\", os.path.join(MODEL_DIR, \"scaler_features.pkl\"))\n",
        "\n",
        "\n",
        "# Cell 9: XGBoost training on engineered features\n",
        "# (Using XGBoost with default parameters for demonstration)\n",
        "xgb = XGBClassifier(objective='multi:softmax', num_class=n_classes, random_state=SEED)\n",
        "xgb.fit(Xf_train_s, yw_train)\n",
        "xgb_pred = xgb.predict(Xf_test_s)\n",
        "print(\"XGBoost Test accuracy:\", accuracy_score(yw_test, xgb_pred))\n",
        "print(classification_report(yw_test, xgb_pred, target_names=le.classes_))\n",
        "xgb.save_model(os.path.join(MODEL_DIR, \"xgb_feats.json\")) # Use json format for compatibility\n",
        "print(\"Saved XGBoost model to\", os.path.join(MODEL_DIR, \"xgb_feats.json\"))\n",
        "\n",
        "\n",
        "# Cell 10: MLP training on engineered features\n",
        "n_feats = Xf_train_s.shape[1]\n",
        "def build_mlp(n_feats, n_classes):\n",
        "    inp = layers.Input(shape=(n_feats,))\n",
        "    x = layers.Dense(256, activation='relu')(inp)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    out = layers.Dense(n_classes, activation='softmax')(x)\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "y_train_cat = to_categorical(yw_train, num_classes=n_classes)\n",
        "y_test_cat  = to_categorical(yw_test,  num_classes=n_classes)\n",
        "\n",
        "mlp = build_mlp(n_feats, n_classes)\n",
        "es = callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
        "mlp.fit(Xf_train_s, y_train_cat, validation_split=0.15, epochs=60, batch_size=128, callbacks=[es], verbose=2)\n",
        "mlp_pred = np.argmax(mlp.predict(Xf_test_s), axis=1)\n",
        "print(\"MLP Test accuracy:\", accuracy_score(yw_test, mlp_pred))\n",
        "print(classification_report(yw_test, mlp_pred, target_names=le.classes_))\n",
        "mlp.save(os.path.join(MODEL_DIR, \"mlp_feats.h5\"))\n",
        "\n",
        "\n",
        "# Cell 11: Soft-vote ensemble (XGB + MLP)\n",
        "xgb_proba = xgb.predict_proba(Xf_test_s)\n",
        "mlp_proba = mlp.predict(Xf_test_s)\n",
        "ens_proba = (xgb_proba + mlp_proba) / 2.0\n",
        "ens_pred = np.argmax(ens_proba, axis=1)\n",
        "print(\"Ensemble (XGB+MLP) Test accuracy:\", accuracy_score(yw_test, ens_pred))\n",
        "print(classification_report(yw_test, ens_pred, target_names=le.classes_))\n",
        "\n",
        "# Save ensemble artifacts\n",
        "joblib.dump(le, os.path.join(MODEL_DIR, \"label_encoder.pkl\"))\n",
        "print(\"Saved label encoder:\", os.path.join(MODEL_DIR, \"label_encoder.pkl\"))\n",
        "\n",
        "\n",
        "# Cell 12: Inference helper (use saved models)\n",
        "def time_feats_single(window):\n",
        "    return time_feats(window)\n",
        "\n",
        "def freq_feats_single(window):\n",
        "    return freq_feats(window, n_bands=6)\n",
        "\n",
        "def predict_from_raw_row_no_cnn(raw_row):\n",
        "    \"\"\"raw_row: 1D np.array of sensor values in order sensor_cols\"\"\"\n",
        "    w = np.tile(np.array(raw_row, dtype=float).reshape(1,-1), (TIMESTEPS,1))\n",
        "    feats = np.concatenate([time_feats_single(w), freq_feats_single(w)])\n",
        "    feats_s = scaler_feats.transform(feats.reshape(1,-1))\n",
        "    p_xgb = xgb.predict_proba(feats_s)[0]\n",
        "    p_mlp = mlp.predict(feats_s)[0]\n",
        "    p_avg = (p_xgb + p_mlp)/2.0\n",
        "    idx = np.argmax(p_avg)\n",
        "    return le.inverse_transform([idx])[0], float(p_avg[idx])\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def predict_from_raw_row_safe(raw_row):\n",
        "    \"\"\"raw_row: [Vibration_X, Vibration_Y, Vibration_Z, Acoustic_Level, Temperature]\"\"\"\n",
        "    # 1. build artificial window (same as before)\n",
        "    w = np.tile(np.array(raw_row, dtype=float).reshape(1, -1), (TIMESTEPS, 1))\n",
        "\n",
        "    # 2. compute time + freq features using your original functions\n",
        "    t_feats = time_feats(w)\n",
        "    f_feats = freq_feats(w, n_bands=6)\n",
        "    feats = np.concatenate([t_feats, f_feats])\n",
        "\n",
        "    # 3. replace NaN / inf with 0 before scaling\n",
        "    feats = np.nan_to_num(feats, nan=0.0, posinf=0.0, neginf=0.0);\n",
        "\n",
        "    # 4. scale and predict (same as before)\n",
        "    feats_s = scaler_feats.transform(feats.reshape(1, -1))\n",
        "    p_xgb = xgb.predict_proba(feats_s)[0]\n",
        "    p_mlp = mlp.predict(feats_s)[0]\n",
        "    p_avg = (p_xgb + p_mlp) / 2.0\n",
        "\n",
        "    idx = int(np.argmax(p_avg))\n",
        "    label = le.inverse_transform([idx])[0]\n",
        "    prob = float(p_avg[idx])\n",
        "\n",
        "    return label, prob\n",
        "\n",
        "\n",
        "# Cell 13: Quick sample prediction + list saved artifacts\n",
        "sample_row = df.loc[100, sensor_cols].values\n",
        "plabel, pprob = predict_from_raw_row_no_cnn(sample_row)\n",
        "print(\"Sample prediction:\", plabel, pprob)\n",
        "print(\"Saved files:\", os.listdir(MODEL_DIR))\n",
        "\n",
        "\n",
        "# Cell 14: Optional - XGBoost feature importance (requires matplotlib)\n",
        "importances = xgb.feature_importances_\n",
        "idx = np.argsort(importances)[::-1][:30]\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.barh(range(len(idx)), importances[idx][::-1])\n",
        "plt.yticks(range(len(idx)), [f\"feat_{i}\" for i in idx[::-1]])\n",
        "plt.title(\"Top feature importances (XGBoost)\")\n",
        "plt.show()\n",
        "\n",
        "def test_custom(vx, vy, vz, acoustic, temp):\n",
        "    raw = [vx, vy, vz, acoustic, temp]\n",
        "    label, prob = predict_from_raw_row_safe(raw)\n",
        "    print(f\"Input: Vx={vx}, Vy={vy}, Vz={vz}, Acoustic={acoustic}, Temp={temp}\")\n",
        "    print(f\" -> Fault: {label}, Confidence: {prob*100:.2f}%\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "test_custom(0.1, 0.2, 0.3, 88, 90)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e40f701"
      },
      "source": [
        "import os, random, joblib, math\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from scipy.stats import skew, kurtosis\n",
        "from scipy.fft import rfft, rfftfreq\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
        "MODEL_DIR = \"/content/models_nocnn\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "DATA_PATH = \"/content/Rotating_equipment_fault_data.csv\"\n",
        "print(\"Loading:\", DATA_PATH)\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"Shape:\", df.shape)\n",
        "display(df.head())\n",
        "\n",
        "\n",
        "# Cell 4: Column detection & cleaning\n",
        "target_col = \"Fault_Type\"\n",
        "sensor_cols = ['Vibration_X','Vibration_Y','Vibration_Z','Acoustic_Level','Temperature']  # from your CSV sample\n",
        "\n",
        "# drop NA rows in required columns\n",
        "df = df.dropna(subset=sensor_cols + [target_col]).reset_index(drop=True)\n",
        "print(\"After dropna:\", df.shape)\n",
        "\n",
        "# label encode target\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df[target_col].values)\n",
        "X_raw = df[sensor_cols].astype(float).values\n",
        "n_classes = len(le.classes_)\n",
        "print(\"Detected classes:\", le.classes_)\n",
        "\n",
        "\n",
        "# Cell 5: Create non-overlapping windows\n",
        "TIMESTEPS = 20  # tune if needed\n",
        "\n",
        "def make_non_overlapping(X, y, timesteps):\n",
        "    Xw, yw = [], []\n",
        "    i = 0\n",
        "    while i + timesteps <= len(X):\n",
        "        Xw.append(X[i:i+timesteps])\n",
        "        yw.append(y[i+timesteps-1])\n",
        "        i += timesteps\n",
        "    return np.array(Xw), np.array(yw)\n",
        "\n",
        "Xw_all, yw_all = make_non_overlapping(X_raw, y, TIMESTEPS)\n",
        "print(\"Windows shape (non-overlap):\", Xw_all.shape, yw_all.shape)\n",
        "\n",
        "\n",
        "# Cell 6: Time-ordered split (train = first 80%, test = last 20%)\n",
        "n = len(Xw_all)\n",
        "split_idx = int(0.8 * n)\n",
        "Xw_train, Xw_test = Xw_all[:split_idx], Xw_all[split_idx:]\n",
        "yw_train, yw_test = yw_all[:split_idx], yw_all[split_idx:]\n",
        "print(\"Train windows:\", Xw_train.shape, \"Test windows:\", Xw_test.shape)\n",
        "\n",
        "\n",
        "# Cell 7: Feature engineering functions\n",
        "def time_feats(window):\n",
        "    feats = []\n",
        "    for ch in range(window.shape[1]):\n",
        "        arr = window[:, ch]\n",
        "        feats += [\n",
        "            arr.mean(),\n",
        "            arr.std(ddof=0),\n",
        "            np.sqrt(np.mean(arr**2)),    # RMS\n",
        "            np.max(arr) - np.min(arr),   # P2P\n",
        "            np.percentile(arr,75)-np.percentile(arr,25),  # IQR\n",
        "            skew(arr),\n",
        "            kurtosis(arr),\n",
        "            (np.max(np.abs(arr)) / (np.mean(np.abs(arr))+1e-9)),  # crest-ish\n",
        "            np.mean(np.diff(arr))\n",
        "        ]\n",
        "    return np.array(feats)\n",
        "\n",
        "def freq_feats(window, n_bands=6):\n",
        "    feats = []\n",
        "    t = window.shape[0]\n",
        "    fft_idx_edges = np.linspace(0, t//2+1, n_bands+1, dtype=int)\n",
        "    for ch in range(window.shape[1]):\n",
        "        sig = window[:, ch]\n",
        "        fft_vals = np.abs(rfft(sig))\n",
        "        energy = (fft_vals**2).sum() + 1e-9\n",
        "        for b in range(n_bands):\n",
        "            v = fft_vals[fft_idx_edges[b]:fft_idx_edges[b+1]]\n",
        "            feats.append((v**2).sum()/energy)\n",
        "    return np.array(feats)\n",
        "\n",
        "def build_feats(Xw):\n",
        "    out = []\n",
        "    for w in Xw:\n",
        "        feat = np.concatenate([time_feats(w), freq_feats(w, n_bands=6)])\n",
        "        out.append(feat)\n",
        "    return np.vstack(out)\n",
        "\n",
        "\n",
        "# Cell 8: Build features & scale (fit scaler on train only)\n",
        "Xf_train = build_feats(Xw_train)\n",
        "Xf_test  = build_feats(Xw_test)\n",
        "print(\"Engineered feature shapes:\", Xf_train.shape, Xf_test.shape)\n",
        "\n",
        "scaler_feats = StandardScaler().fit(Xf_train)\n",
        "Xf_train_s = scaler_feats.transform(Xf_train)\n",
        "Xf_test_s  = scaler_feats.transform(Xf_test)\n",
        "joblib.dump(scaler_feats, os.path.join(MODEL_DIR, \"scaler_features.pkl\"))\n",
        "print(\"Saved scaler to\", os.path.join(MODEL_DIR, \"scaler_features.pkl\"))\n",
        "\n",
        "\n",
        "# Cell 9: XGBoost training on engineered features\n",
        "# (Using XGBoost with default parameters for demonstration)\n",
        "xgb = XGBClassifier(objective='multi:softmax', num_class=n_classes, random_state=SEED)\n",
        "xgb.fit(Xf_train_s, yw_train)\n",
        "xgb_pred = xgb.predict(Xf_test_s)\n",
        "print(\"XGBoost Test accuracy:\", accuracy_score(yw_test, xgb_pred))\n",
        "print(classification_report(yw_test, xgb_pred, target_names=le.classes_))\n",
        "xgb.save_model(os.path.join(MODEL_DIR, \"xgb_feats.json\")) # Use json format for compatibility\n",
        "print(\"Saved XGBoost model to\", os.path.join(MODEL_DIR, \"xgb_feats.json\"))\n",
        "\n",
        "\n",
        "# Cell 10: MLP training on engineered features\n",
        "n_feats = Xf_train_s.shape[1]\n",
        "def build_mlp(n_feats, n_classes):\n",
        "    inp = layers.Input(shape=(n_feats,))\n",
        "    x = layers.Dense(256, activation='relu')(inp)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    out = layers.Dense(n_classes, activation='softmax')(x)\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "y_train_cat = to_categorical(yw_train, num_classes=n_classes)\n",
        "y_test_cat  = to_categorical(yw_test,  num_classes=n_classes)\n",
        "\n",
        "mlp = build_mlp(n_feats, n_classes)\n",
        "es = callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
        "mlp.fit(Xf_train_s, y_train_cat, validation_split=0.15, epochs=60, batch_size=128, callbacks=[es], verbose=2)\n",
        "mlp_pred = np.argmax(mlp.predict(Xf_test_s), axis=1)\n",
        "print(\"MLP Test accuracy:\", accuracy_score(yw_test, mlp_pred))\n",
        "print(classification_report(yw_test, mlp_pred, target_names=le.classes_))\n",
        "mlp.save(os.path.join(MODEL_DIR, \"mlp_feats.h5\"))\n",
        "\n",
        "\n",
        "# Cell 11: Soft-vote ensemble (XGB + MLP)\n",
        "xgb_proba = xgb.predict_proba(Xf_test_s)\n",
        "mlp_proba = mlp.predict(Xf_test_s)\n",
        "ens_proba = (xgb_proba + mlp_proba) / 2.0\n",
        "ens_pred = np.argmax(ens_proba, axis=1)\n",
        "print(\"Ensemble (XGB+MLP) Test accuracy:\", accuracy_score(yw_test, ens_pred))\n",
        "print(classification_report(yw_test, ens_pred, target_names=le.classes_))\n",
        "\n",
        "# Save ensemble artifacts\n",
        "joblib.dump(le, os.path.join(MODEL_DIR, \"label_encoder.pkl\"))\n",
        "print(\"Saved label encoder:\", os.path.join(MODEL_DIR, \"label_encoder.pkl\"))\n",
        "\n",
        "\n",
        "# Cell 12: Inference helper (use saved models)\n",
        "def time_feats_single(window):\n",
        "    return time_feats(window)\n",
        "\n",
        "def freq_feats_single(window):\n",
        "    return freq_feats(window, n_bands=6)\n",
        "\n",
        "def predict_from_raw_row_no_cnn(raw_row):\n",
        "    \"\"\"raw_row: 1D np.array of sensor values in order sensor_cols\"\"\"\n",
        "    w = np.tile(np.array(raw_row, dtype=float).reshape(1,-1), (TIMESTEPS,1))\n",
        "    feats = np.concatenate([time_feats_single(w), freq_feats_single(w)])\n",
        "    feats_s = scaler_feats.transform(feats.reshape(1,-1))\n",
        "    p_xgb = xgb.predict_proba(feats_s)[0]\n",
        "    p_mlp = mlp.predict(feats_s)[0]\n",
        "    p_avg = (p_xgb + p_mlp)/2.0\n",
        "    idx = np.argmax(p_avg)\n",
        "    return le.inverse_transform([idx])[0], float(p_avg[idx])\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def predict_from_raw_row_safe(raw_row):\n",
        "    \"\"\"raw_row: [Vibration_X, Vibration_Y, Vibration_Z, Acoustic_Level, Temperature]\"\"\"\n",
        "    # 1. build artificial window (same as before)\n",
        "    w = np.tile(np.array(raw_row, dtype=float).reshape(1, -1), (TIMESTEPS, 1))\n",
        "\n",
        "    # 2. compute time + freq features using your original functions\n",
        "    t_feats = time_feats(w)\n",
        "    f_feats = freq_feats(w, n_bands=6)\n",
        "    feats = np.concatenate([t_feats, f_feats])\n",
        "\n",
        "    # 3. replace NaN / inf with 0 before scaling\n",
        "    feats = np.nan_to_num(feats, nan=0.0, posinf=0.0, neginf=0.0);\n",
        "\n",
        "    # 4. scale and predict (same as before)\n",
        "    feats_s = scaler_feats.transform(feats.reshape(1, -1))\n",
        "    p_xgb = xgb.predict_proba(feats_s)[0]\n",
        "    p_mlp = mlp.predict(feats_s)[0]\n",
        "    p_avg = (p_xgb + p_mlp) / 2.0\n",
        "\n",
        "    idx = int(np.argmax(p_avg))\n",
        "    label = le.inverse_transform([idx])[0]\n",
        "    prob = float(p_avg[idx])\n",
        "\n",
        "    return label, prob\n",
        "\n",
        "\n",
        "# Cell 13: Quick sample prediction + list saved artifacts\n",
        "sample_row = df.loc[100, sensor_cols].values\n",
        "plabel, pprob = predict_from_raw_row_no_cnn(sample_row)\n",
        "print(\"Sample prediction:\", plabel, pprob)\n",
        "print(\"Saved files:\", os.listdir(MODEL_DIR))\n",
        "\n",
        "\n",
        "# Cell 14: Optional - XGBoost feature importance (requires matplotlib)\n",
        "importances = xgb.feature_importances_\n",
        "idx = np.argsort(importances)[::-1][:30]\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.barh(range(len(idx)), importances[idx][::-1])\n",
        "plt.yticks(range(len(idx)), [f\"feat_{i}\" for i in idx[::-1]])\n",
        "plt.title(\"Top feature importances (XGBoost)\")\n",
        "plt.show()\n",
        "\n",
        "def test_custom(vx, vy, vz, acoustic, temp):\n",
        "    raw = [vx, vy, vz, acoustic, temp]\n",
        "    label, prob = predict_from_raw_row_safe(raw)\n",
        "    print(f\"Input: Vx={vx}, Vy={vy}, Vz={vz}, Acoustic={acoustic}, Temp={temp}\")\n",
        "    print(f\" -> Fault: {label}, Confidence: {prob*100:.2f}%\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "test_custom(0.1, 0.2, 0.3, 88, 90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3af7c62"
      },
      "source": [
        "```python\n",
        "it\n",
        "\n",
        "!pip install -q xgboost imbalanced-learn shap\n",
        "```\n",
        "```python\n",
        "import os, random, joblib, math\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from scipy.stats import skew, kurtosis\n",
        "from scipy.fft import rfft, rfftfreq\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
        "MODEL_DIR = \"/content/models_nocnn\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "```\n",
        "```python\n",
        "DATA_PATH = \"/content/Rotating_equipment_fault_data.csv\"\n",
        "print(\"Loading:\", DATA_PATH)\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"Shape:\", df.shape)\n",
        "display(df.head())\n",
        "```\n",
        "```python\n",
        "# Cell 4: Column detection & cleaning\n",
        "target_col = \"Fault_Type\"\n",
        "sensor_cols = ['Vibration_X','Vibration_Y','Vibration_Z','Acoustic_Level','Temperature']  # from your CSV sample\n",
        "\n",
        "# drop NA rows in required columns\n",
        "df = df.dropna(subset=sensor_cols + [target_col]).reset_index(drop=True)\n",
        "print(\"After dropna:\", df.shape)\n",
        "\n",
        "# label encode target\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df[target_col].values)\n",
        "X_raw = df[sensor_cols].astype(float).values\n",
        "n_classes = len(le.classes_)\n",
        "print(\"Detected classes:\", le.classes_)\n",
        "```\n",
        "```python\n",
        "# Cell 5: Create non-overlapping windows\n",
        "TIMESTEPS = 20  # tune if needed\n",
        "\n",
        "def make_non_overlapping(X, y, timesteps):\n",
        "    Xw, yw = [], []\n",
        "    i = 0\n",
        "    while i + timesteps <= len(X):\n",
        "        Xw.append(X[i:i+timesteps])\n",
        "        yw.append(y[i+timesteps-1])\n",
        "        i += timesteps\n",
        "    return np.array(Xw), np.array(yw)\n",
        "\n",
        "Xw_all, yw_all = make_non_overlapping(X_raw, y, TIMESTEPS)\n",
        "print(\"Windows shape (non-overlap):\", Xw_all.shape, yw_all.shape)\n",
        "```\n",
        "```python\n",
        "# Cell 6: Time-ordered split (train = first 80%, test = last 20%)\n",
        "n = len(Xw_all)\n",
        "split_idx = int(0.8 * n)\n",
        "Xw_train, Xw_test = Xw_all[:split_idx], Xw_all[split_idx:]\n",
        "yw_train, yw_test = yw_all[:split_idx], yw_all[split_idx:]\n",
        "print(\"Train windows:\", Xw_train.shape, \"Test windows:\", Xw_test.shape)\n",
        "```\n",
        "```python\n",
        "# Cell 7: Feature engineering functions\n",
        "def time_feats(window):\n",
        "    feats = []\n",
        "    for ch in range(window.shape[1]):\n",
        "        arr = window[:, ch]\n",
        "        feats += [\n",
        "            arr.mean(),\n",
        "            arr.std(ddof=0),\n",
        "            np.sqrt(np.mean(arr**2)),    # RMS\n",
        "            np.max(arr) - np.min(arr),   # P2P\n",
        "            np.percentile(arr,75)-np.percentile(arr,25),  # IQR\n",
        "            skew(arr),\n",
        "            kurtosis(arr),\n",
        "            (np.max(np.abs(arr)) / (np.mean(np.abs(arr))+1e-9)),  # crest-ish\n",
        "            np.mean(np.diff(arr))\n",
        "        ]\n",
        "    return np.array(feats)\n",
        "\n",
        "def freq_feats(window, n_bands=6):\n",
        "    feats = []\n",
        "    t = window.shape[0]\n",
        "    fft_idx_edges = np.linspace(0, t//2+1, n_bands+1, dtype=int)\n",
        "    for ch in range(window.shape[1]):\n",
        "        sig = window[:, ch]\n",
        "        fft_vals = np.abs(rfft(sig))\n",
        "        energy = (fft_vals**2).sum() + 1e-9\n",
        "        for b in range(n_bands):\n",
        "            v = fft_vals[fft_idx_edges[b]:fft_idx_edges[b+1]]\n",
        "            feats.append((v**2).sum()/energy)\n",
        "    return np.array(feats)\n",
        "\n",
        "def build_feats(Xw):\n",
        "    out = []\n",
        "    for w in Xw:\n",
        "        feat = np.concatenate([time_feats(w), freq_feats(w, n_bands=6)])\n",
        "        out.append(feat)\n",
        "    return np.vstack(out)\n",
        "```\n",
        "```python\n",
        "# Cell 8: Build features & scale (fit scaler on train only)\n",
        "Xf_train = build_feats(Xw_train)\n",
        "Xf_test  = build_feats(Xw_test)\n",
        "print(\"Engineered feature shapes:\", Xf_train.shape, Xf_test.shape)\n",
        "\n",
        "scaler_feats = StandardScaler().fit(Xf_train)\n",
        "Xf_train_s = scaler_feats.transform(Xf_train)\n",
        "Xf_test_s  = scaler_feats.transform(Xf_test)\n",
        "joblib.dump(scaler_feats, os.path.join(MODEL_DIR, \"scaler_features.pkl\"))\n",
        "print(\"Saved scaler to\", os.path.join(MODEL_DIR, \"scaler_features.pkl\"))\n",
        "```\n",
        "```python\n",
        "# Cell 9: XGBoost training on engineered features\n",
        "# (Using XGBoost with default parameters for demonstration)\n",
        "xgb = XGBClassifier(objective='multi:softmax', num_class=n_classes, random_state=SEED)\n",
        "xgb.fit(Xf_train_s, yw_train)\n",
        "xgb_pred = xgb.predict(Xf_test_s)\n",
        "print(\"XGBoost Test accuracy:\", accuracy_score(yw_test, xgb_pred))\n",
        "print(classification_report(yw_test, xgb_pred, target_names=le.classes_))\n",
        "xgb.save_model(os.path.join(MODEL_DIR, \"xgb_feats.json\")) # Use json format for compatibility\n",
        "print(\"Saved XGBoost model to\", os.path.join(MODEL_DIR, \"xgb_feats.json\"))\n",
        "```\n",
        "```python\n",
        "# Cell 10: MLP training on engineered features\n",
        "n_feats = Xf_train_s.shape[1]\n",
        "def build_mlp(n_feats, n_classes):\n",
        "    inp = layers.Input(shape=(n_feats,))\n",
        "    x = layers.Dense(256, activation='relu')(inp)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    out = layers.Dense(n_classes, activation='softmax')(x)\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "y_train_cat = to_categorical(yw_train, num_classes=n_classes)\n",
        "y_test_cat  = to_categorical(yw_test,  num_classes=n_classes)\n",
        "\n",
        "mlp = build_mlp(n_feats, n_classes)\n",
        "es = callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
        "mlp.fit(Xf_train_s, y_train_cat, validation_split=0.15, epochs=60, batch_size=128, callbacks=[es], verbose=2)\n",
        "mlp_pred = np.argmax(mlp.predict(Xf_test_s), axis=1)\n",
        "print(\"MLP Test accuracy:\", accuracy_score(yw_test, mlp_pred))\n",
        "print(classification_report(yw_test, mlp_pred, target_names=le.classes_))\n",
        "mlp.save(os.path.join(MODEL_DIR, \"mlp_feats.h5\"))\n",
        "```\n",
        "```python\n",
        "# Cell 11: Soft-vote ensemble (XGB + MLP)\n",
        "xgb_proba = xgb.predict_proba(Xf_test_s)\n",
        "mlp_proba = mlp.predict(Xf_test_s)\n",
        "ens_proba = (xgb_proba + mlp_proba) / 2.0\n",
        "ens_pred = np.argmax(ens_proba, axis=1)\n",
        "print(\"Ensemble (XGB+MLP) Test accuracy:\", accuracy_score(yw_test, ens_pred))\n",
        "print(classification_report(yw_test, ens_pred, target_names=le.classes_))\n",
        "\n",
        "# Save ensemble artifacts\n",
        "joblib.dump(le, os.path.join(MODEL_DIR, \"label_encoder.pkl\"))\n",
        "print(\"Saved label encoder:\", os.path.join(MODEL_DIR, \"label_encoder.pkl\"))\n",
        "```\n",
        "```python\n",
        "# Cell 12: Inference helper (use saved models)\n",
        "def time_feats_single(window):\n",
        "    return time_feats(window)\n",
        "\n",
        "def freq_feats_single(window):\n",
        "    return freq_feats(window, n_bands=6)\n",
        "\n",
        "def predict_from_raw_row_no_cnn(raw_row):\n",
        "    \"\"\"raw_row: 1D np.array of sensor values in order sensor_cols\"\"\"\n",
        "    w = np.tile(np.array(raw_row, dtype=float).reshape(1,-1), (TIMESTEPS,1))\n",
        "    feats = np.concatenate([time_feats_single(w), freq_feats_single(w)])\n",
        "    feats_s = scaler_feats.transform(feats.reshape(1,-1))\n",
        "    p_xgb = xgb.predict_proba(feats_s)[0]\n",
        "    p_mlp = mlp.predict(feats_s)[0]\n",
        "    p_avg = (p_xgb + p_mlp)/2.0\n",
        "    idx = np.argmax(p_avg)\n",
        "    return le.inverse_transform([idx])[0], float(p_avg[idx])\n",
        "```\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "def predict_from_raw_row_safe(raw_row):\n",
        "    \"\"\"raw_row: [Vibration_X, Vibration_Y, Vibration_Z, Acoustic_Level, Temperature]\"\"\"\n",
        "    # 1. build artificial window (same as before)\n",
        "    w = np.tile(np.array(raw_row, dtype=float).reshape(1, -1), (TIMESTEPS, 1))\n",
        "\n",
        "    # 2. compute time + freq features using your original functions\n",
        "    t_feats = time_feats(w)\n",
        "    f_feats = freq_feats(w, n_bands=6)\n",
        "    feats = np.concatenate([t_feats, f_feats])\n",
        "\n",
        "    # 3. replace NaN / inf with 0 before scaling\n",
        "    feats = np.nan_to_num(feats, nan=0.0, posinf=0.0, neginf=0.0);\n",
        "\n",
        "    # 4. scale and predict (same as before)\n",
        "    feats_s = scaler_feats.transform(feats.reshape(1, -1))\n",
        "    p_xgb = xgb.predict_proba(feats_s)[0]\n",
        "    p_mlp = mlp.predict(feats_s)[0]\n",
        "    p_avg = (p_xgb + p_mlp) / 2.0\n",
        "\n",
        "    idx = int(np.argmax(p_avg))\n",
        "    label = le.inverse_transform([idx])[0]\n",
        "    prob = float(p_avg[idx])\n",
        "\n",
        "    return label, prob\n",
        "```\n",
        "```python\n",
        "# Cell 13: Quick sample prediction + list saved artifacts\n",
        "sample_row = df.loc[100, sensor_cols].values\n",
        "plabel, pprob = predict_from_raw_row_no_cnn(sample_row)\n",
        "print(\"Sample prediction:\", plabel, pprob)\n",
        "print(\"Saved files:\", os.listdir(MODEL_DIR))\n",
        "```\n",
        "```python\n",
        "# Cell 14: Optional - XGBoost feature importance (requires matplotlib)\n",
        "importances = xgb.feature_importances_\n",
        "idx = np.argsort(importances)[::-1][:30]\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.barh(range(len(idx)), importances[idx][::-1])\n",
        "plt.yticks(range(len(idx)), [f\"feat_{i}\" for i in idx[::-1]])\n",
        "plt.title(\"Top feature importances (XGBoost)\")\n",
        "plt.show()\n",
        "```\n",
        "```python\n",
        "def test_custom(vx, vy, vz, acoustic, temp):\n",
        "    raw = [vx, vy, vz, acoustic, temp]\n",
        "    label, prob = predict_from_raw_row_safe(raw)\n",
        "    print(f\"Input: Vx={vx}, Vy={vy}, Vz={vz}, Acoustic={acoustic}, Temp={temp}\")\n",
        "    print(f\" -> Fault: {label}, Confidence: {prob*100:.2f}%\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "test_custom(0.1, 0.2, 0.3, 88, 90)\n",
        "```\n",
        "```python\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oni5u4egfVm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87d65229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "26943689-9e41-4d51-817f-6a74264e7c84"
      },
      "source": [
        "import os, random, joblib, math\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from scipy.stats import skew, kurtosis\n",
        "from scipy.fft import rfft, rfftfreq\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
        "MODEL_DIR = \"/content/models_nocnn\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "DATA_PATH = \"/content/Rotating_equipment_fault_data.csv\"\n",
        "print(\"Loading:\", DATA_PATH)\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"Shape:\", df.shape)\n",
        "#display(df.head()) # Commented out as display() is for interactive environments\n",
        "\n",
        "# Cell 4: Column detection & cleaning\n",
        "target_col = \"Fault_Type\"\n",
        "sensor_cols = ['Vibration_X','Vibration_Y','Vibration_Z','Acoustic_Level','Temperature']  # from your CSV sample\n",
        "\n",
        "# drop NA rows in required columns\n",
        "df = df.dropna(subset=sensor_cols + [target_col]).reset_index(drop=True)\n",
        "print(\"After dropna:\", df.shape)\n",
        "\n",
        "# label encode target\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df[target_col].values)\n",
        "X_raw = df[sensor_cols].astype(float).values\n",
        "n_classes = len(le.classes_)\n",
        "print(\"Detected classes:\", le.classes_)\n",
        "\n",
        "# Cell 5: Create non-overlapping windows\n",
        "TIMESTEPS = 20  # tune if needed\n",
        "\n",
        "def make_non_overlapping(X, y, timesteps):\n",
        "    Xw, yw = [], []\n",
        "    i = 0\n",
        "    while i + timesteps <= len(X):\n",
        "        Xw.append(X[i:i+timesteps])\n",
        "        yw.append(y[i+timesteps-1])\n",
        "        i += timesteps\n",
        "    return np.array(Xw), np.array(yw)\n",
        "\n",
        "Xw_all, yw_all = make_non_overlapping(X_raw, y, TIMESTEPS)\n",
        "print(\"Windows shape (non-overlap):\", Xw_all.shape, yw_all.shape)\n",
        "\n",
        "# Cell 6: Time-ordered split (train = first 80%, test = last 20%)\n",
        "n = len(Xw_all)\n",
        "split_idx = int(0.8 * n)\n",
        "Xw_train, Xw_test = Xw_all[:split_idx], Xw_all[split_idx:]\n",
        "yw_train, yw_test = yw_all[:split_idx], yw_all[split_idx:]\n",
        "print(\"Train windows:\", Xw_train.shape, \"Test windows:\", Xw_test.shape)\n",
        "\n",
        "# Cell 7: Feature engineering functions\n",
        "def time_feats(window):\n",
        "    feats = []\n",
        "    for ch in range(window.shape[1]):\n",
        "        arr = window[:, ch]\n",
        "        feats += [\n",
        "            arr.mean(),\n",
        "            arr.std(ddof=0),\n",
        "            np.sqrt(np.mean(arr**2)),    # RMS\n",
        "            np.max(arr) - np.min(arr),   # P2P\n",
        "            np.percentile(arr,75)-np.percentile(arr,25),  # IQR\n",
        "            skew(arr),\n",
        "            kurtosis(arr),\n",
        "            (np.max(np.abs(arr)) / (np.mean(np.abs(arr))+1e-9)),  # crest-ish\n",
        "            np.mean(np.diff(arr))\n",
        "        ]\n",
        "    return np.array(feats)\n",
        "\n",
        "def freq_feats(window, n_bands=6):\n",
        "    feats = []\n",
        "    t = window.shape[0]\n",
        "    fft_idx_edges = np.linspace(0, t//2+1, n_bands+1, dtype=int)\n",
        "    for ch in range(window.shape[1]):\n",
        "        sig = window[:, ch]\n",
        "        fft_vals = np.abs(rfft(sig))\n",
        "        energy = (fft_vals**2).sum() + 1e-9\n",
        "        for b in range(n_bands):\n",
        "            v = fft_vals[fft_idx_edges[b]:fft_idx_edges[b+1]]\n",
        "            feats.append((v**2).sum()/energy)\n",
        "    return np.array(feats)\n",
        "\n",
        "def build_feats(Xw):\n",
        "    out = []\n",
        "    for w in Xw:\n",
        "        feat = np.concatenate([time_feats(w), freq_feats(w, n_bands=6)])\n",
        "        out.append(feat)\n",
        "    return np.vstack(out)\n",
        "\n",
        "# Cell 8: Build features & scale (fit scaler on train only)\n",
        "Xf_train = build_feats(Xw_train)\n",
        "Xf_test  = build_feats(Xw_test)\n",
        "print(\"Engineered feature shapes:\", Xf_train.shape, Xf_test.shape)\n",
        "\n",
        "scaler_feats = StandardScaler().fit(Xf_train)\n",
        "Xf_train_s = scaler_feats.transform(Xf_train)\n",
        "Xf_test_s  = scaler_feats.transform(Xf_test)\n",
        "joblib.dump(scaler_feats, os.path.join(MODEL_DIR, \"scaler_features.pkl\"))\n",
        "print(\"Saved scaler to\", os.path.join(MODEL_DIR, \"scaler_features.pkl\"))\n",
        "\n",
        "# Cell 9: XGBoost training on engineered features\n",
        "# (Using XGBoost with default parameters for demonstration)\n",
        "xgb = XGBClassifier(objective='multi:softmax', num_class=n_classes, random_state=SEED)\n",
        "xgb.fit(Xf_train_s, yw_train)\n",
        "xgb_pred = xgb.predict(Xf_test_s)\n",
        "print(\"XGBoost Test accuracy:\", accuracy_score(yw_test, xgb_pred))\n",
        "print(classification_report(yw_test, xgb_pred, target_names=le.classes_))\n",
        "xgb.save_model(os.path.join(MODEL_DIR, \"xgb_feats.json\")) # Use json format for compatibility\n",
        "print(\"Saved XGBoost model to\", os.path.join(MODEL_DIR, \"xgb_feats.json\"))\n",
        "\n",
        "# Cell 10: MLP training on engineered features\n",
        "n_feats = Xf_train_s.shape[1]\n",
        "def build_mlp(n_feats, n_classes):\n",
        "    inp = layers.Input(shape=(n_feats,))\n",
        "    x = layers.Dense(256, activation='relu')(inp)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    out = layers.Dense(n_classes, activation='softmax')(x)\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "y_train_cat = to_categorical(yw_train, num_classes=n_classes)\n",
        "y_test_cat  = to_categorical(yw_test,  num_classes=n_classes)\n",
        "\n",
        "mlp = build_mlp(n_feats, n_classes)\n",
        "es = callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
        "mlp.fit(Xf_train_s, y_train_cat, validation_split=0.15, epochs=60, batch_size=128, callbacks=[es], verbose=2)\n",
        "mlp_pred = np.argmax(mlp.predict(Xf_test_s), axis=1)\n",
        "print(\"MLP Test accuracy:\", accuracy_score(yw_test, mlp_pred))\n",
        "print(classification_report(yw_test, mlp_pred, target_names=le.classes_))\n",
        "mlp.save(os.path.join(MODEL_DIR, \"mlp_feats.h5\"))\n",
        "\n",
        "# Cell 11: Soft-vote ensemble (XGB + MLP)\n",
        "xgb_proba = xgb.predict_proba(Xf_test_s)\n",
        "mlp_proba = mlp.predict(Xf_test_s)\n",
        "ens_proba = (xgb_proba + mlp_proba) / 2.0\n",
        "ens_pred = np.argmax(ens_proba, axis=1)\n",
        "print(\"Ensemble (XGB+MLP) Test accuracy:\", accuracy_score(yw_test, ens_pred))\n",
        "print(classification_report(yw_test, ens_pred, target_names=le.classes_))\n",
        "\n",
        "# Save ensemble artifacts\n",
        "joblib.dump(le, os.path.join(MODEL_DIR, \"label_encoder.pkl\"))\n",
        "print(\"Saved label encoder:\", os.path.join(MODEL_DIR, \"label_encoder.pkl\"))\n",
        "\n",
        "# Cell 12: Inference helper (use saved models)\n",
        "def time_feats_single(window):\n",
        "    return time_feats(window)\n",
        "\n",
        "def freq_feats_single(window):\n",
        "    return freq_feats(window, n_bands=6)\n",
        "\n",
        "def predict_from_raw_row_no_cnn(raw_row):\n",
        "    \"\"\"raw_row: 1D np.array of sensor values in order sensor_cols\"\"\"\n",
        "    w = np.tile(np.array(raw_row, dtype=float).reshape(1,-1), (TIMESTEPS,1))\n",
        "    feats = np.concatenate([time_feats_single(w), freq_feats_single(w)])\n",
        "    feats_s = scaler_feats.transform(feats.reshape(1,-1))\n",
        "    p_xgb = xgb.predict_proba(feats_s)[0]\n",
        "    p_mlp = mlp.predict(feats_s)[0]\n",
        "    p_avg = (p_xgb + p_mlp)/2.0\n",
        "    idx = np.argmax(p_avg)\n",
        "    return le.inverse_transform([idx])[0], float(p_avg[idx])\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def predict_from_raw_row_safe(raw_row):\n",
        "    \"\"\"raw_row: [Vibration_X, Vibration_Y, Vibration_Z, Acoustic_Level, Temperature]\"\"\"\n",
        "    # 1. build artificial window (same as before)\n",
        "    w = np.tile(np.array(raw_row, dtype=float).reshape(1, -1), (TIMESTEPS, 1))\n",
        "\n",
        "    # 2. compute time + freq features using your original functions\n",
        "    t_feats = time_feats(w)\n",
        "    f_feats = freq_feats(w, n_bands=6)\n",
        "    feats = np.concatenate([t_feats, f_feats])\n",
        "\n",
        "    # 3. replace NaN / inf with 0 before scaling\n",
        "    feats = np.nan_to_num(feats, nan=0.0, posinf=0.0, neginf=0.0);\n",
        "\n",
        "    # 4. scale and predict (same as before)\n",
        "    feats_s = scaler_feats.transform(feats.reshape(1, -1))\n",
        "    p_xgb = xgb.predict_proba(feats_s)[0]\n",
        "    p_mlp = mlp.predict(feats_s)[0]\n",
        "    p_avg = (p_xgb + p_mlp) / 2.0\n",
        "\n",
        "    idx = int(np.argmax(p_avg))\n",
        "    label = le.inverse_transform([idx])[0]\n",
        "    prob = float(p_avg[idx])\n",
        "\n",
        "    return label, prob\n",
        "\n",
        "# Cell 13: Quick sample prediction + list saved artifacts\n",
        "sample_row = df.loc[100, sensor_cols].values\n",
        "plabel, pprob = predict_from_raw_row_no_cnn(sample_row)\n",
        "print(\"Sample prediction:\", plabel, pprob)\n",
        "print(\"Saved files:\", os.listdir(MODEL_DIR))\n",
        "\n",
        "# Cell 14: Optional - XGBoost feature importance (requires matplotlib)\n",
        "#importances = xgb.feature_importances_\n",
        "#idx = np.argsort(importances)[::-1][:30]\n",
        "#plt.figure(figsize=(8,6))\n",
        "#plt.barh(range(len(idx)), importances[idx][::-1])\n",
        "#plt.yticks(range(len(idx)), [f\"feat_{i}\" for i in idx[::-1]])\n",
        "#plt.title(\"Top feature importances (XGBoost)\")\n",
        "#plt.show()\n",
        "\n",
        "def test_custom(vx, vy, vz, acoustic, temp):\n",
        "    raw = [vx, vy, vz, acoustic, temp]\n",
        "    label, prob = predict_from_raw_row_safe(raw)\n",
        "    print(f\"Input: Vx={vx}, Vy={vy}, Vz={vz}, Acoustic={acoustic}, Temp={temp}\")\n",
        "    print(f\" -> Fault: {label}, Confidence: {prob*100:.2f}%\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "test_custom(0.1, 0.2, 0.3, 88, 90)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading: /content/Rotating_equipment_fault_data.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/Rotating_equipment_fault_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2778249064.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mDATA_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/Rotating_equipment_fault_data.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#display(df.head()) # Commented out as display() is for interactive environments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Rotating_equipment_fault_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os, random, joblib, math\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from scipy.stats import skew, kurtosis\n",
        "from scipy.fft import rfft, rfftfreq\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
        "MODEL_DIR = \"/content/models_nocnn\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "c3CbHc5YGzli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DATA_PATH = \"/content/Rotating_equipment_fault_data.csv\"\n",
        "print(\"Loading:\", DATA_PATH)\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"Shape:\", df.shape)\n",
        "display(df.head())\n"
      ],
      "metadata": {
        "id": "jmL2KgxZGzot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Column detection & cleaning\n",
        "target_col = \"Fault_Type\"\n",
        "sensor_cols = ['Vibration_X','Vibration_Y','Vibration_Z','Acoustic_Level','Temperature']  # from your CSV sample\n",
        "\n",
        "# drop NA rows in required columns\n",
        "df = df.dropna(subset=sensor_cols + [target_col]).reset_index(drop=True)\n",
        "print(\"After dropna:\", df.shape)\n",
        "\n",
        "# label encode target\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df[target_col].values)\n",
        "X_raw = df[sensor_cols].astype(float).values\n",
        "n_classes = len(le.classes_)\n",
        "print(\"Detected classes:\", le.classes_)\n"
      ],
      "metadata": {
        "id": "adoN_KhGGzq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Create non-overlapping windows\n",
        "TIMESTEPS = 20  # tune if needed\n",
        "\n",
        "def make_non_overlapping(X, y, timesteps):\n",
        "    Xw, yw = [], []\n",
        "    i = 0\n",
        "    while i + timesteps <= len(X):\n",
        "        Xw.append(X[i:i+timesteps])\n",
        "        yw.append(y[i+timesteps-1])\n",
        "        i += timesteps\n",
        "    return np.array(Xw), np.array(yw)\n",
        "\n",
        "Xw_all, yw_all = make_non_overlapping(X_raw, y, TIMESTEPS)\n",
        "print(\"Windows shape (non-overlap):\", Xw_all.shape, yw_all.shape)\n"
      ],
      "metadata": {
        "id": "cnLpYNcMGzs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Time-ordered split (train = first 80%, test = last 20%)\n",
        "n = len(Xw_all)\n",
        "split_idx = int(0.8 * n)\n",
        "Xw_train, Xw_test = Xw_all[:split_idx], Xw_all[split_idx:]\n",
        "yw_train, yw_test = yw_all[:split_idx], yw_all[split_idx:]\n",
        "print(\"Train windows:\", Xw_train.shape, \"Test windows:\", Xw_test.shape)\n"
      ],
      "metadata": {
        "id": "RJ2nQ90FGzuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Feature engineering functions\n",
        "def time_feats(window):\n",
        "    feats = []\n",
        "    for ch in range(window.shape[1]):\n",
        "        arr = window[:, ch]\n",
        "        feats += [\n",
        "            arr.mean(),\n",
        "            arr.std(ddof=0),\n",
        "            np.sqrt(np.mean(arr**2)),    # RMS\n",
        "            np.max(arr) - np.min(arr),   # P2P\n",
        "            np.percentile(arr,75)-np.percentile(arr,25),  # IQR\n",
        "            skew(arr),\n",
        "            kurtosis(arr),\n",
        "            (np.max(np.abs(arr)) / (np.mean(np.abs(arr))+1e-9)),  # crest-ish\n",
        "            np.mean(np.diff(arr))\n",
        "        ]\n",
        "    return np.array(feats)\n",
        "\n",
        "def freq_feats(window, n_bands=6):\n",
        "    feats = []\n",
        "    t = window.shape[0]\n",
        "    fft_idx_edges = np.linspace(0, t//2+1, n_bands+1, dtype=int)\n",
        "    for ch in range(window.shape[1]):\n",
        "        sig = window[:, ch]\n",
        "        fft_vals = np.abs(rfft(sig))\n",
        "        energy = (fft_vals**2).sum() + 1e-9\n",
        "        for b in range(n_bands):\n",
        "            v = fft_vals[fft_idx_edges[b]:fft_idx_edges[b+1]]\n",
        "            feats.append((v**2).sum()/energy)\n",
        "    return np.array(feats)\n",
        "\n",
        "def build_feats(Xw):\n",
        "    out = []\n",
        "    for w in Xw:\n",
        "        feat = np.concatenate([time_feats(w), freq_feats(w, n_bands=6)])\n",
        "        out.append(feat)\n",
        "    return np.vstack(out)\n"
      ],
      "metadata": {
        "id": "lCAv7n-PGzvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Build features & scale (fit scaler on train only)\n",
        "Xf_train = build_feats(Xw_train)\n",
        "Xf_test  = build_feats(Xw_test)\n",
        "print(\"Engineered feature shapes:\", Xf_train.shape, Xf_test.shape)\n",
        "\n",
        "scaler_feats = StandardScaler().fit(Xf_train)\n",
        "Xf_train_s = scaler_feats.transform(Xf_train)\n",
        "Xf_test_s  = scaler_feats.transform(Xf_test)\n",
        "joblib.dump(scaler_feats, os.path.join(MODEL_DIR, \"scaler_features.pkl\"))\n",
        "print(\"Saved scaler to\", os.path.join(MODEL_DIR, \"scaler_features.pkl\"))\n"
      ],
      "metadata": {
        "id": "d6mPzFMtGzwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: XGBoost training on engineered features\n",
        "# (Using XGBoost with default parameters for demonstration)\n",
        "xgb = XGBClassifier(objective='multi:softmax', num_class=n_classes, random_state=SEED)\n",
        "xgb.fit(Xf_train_s, yw_train)\n",
        "xgb_pred = xgb.predict(Xf_test_s)\n",
        "print(\"XGBoost Test accuracy:\", accuracy_score(yw_test, xgb_pred))\n",
        "print(classification_report(yw_test, xgb_pred, target_names=le.classes_))\n",
        "xgb.save_model(os.path.join(MODEL_DIR, \"xgb_feats.json\")) # Use json format for compatibility\n",
        "print(\"Saved XGBoost model to\", os.path.join(MODEL_DIR, \"xgb_feats.json\"))"
      ],
      "metadata": {
        "id": "hiejnuSKRMiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: MLP training on engineered features\n",
        "n_feats = Xf_train_s.shape[1]\n",
        "def build_mlp(n_feats, n_classes):\n",
        "    inp = layers.Input(shape=(n_feats,))\n",
        "    x = layers.Dense(256, activation='relu')(inp)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    out = layers.Dense(n_classes, activation='softmax')(x)\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "y_train_cat = to_categorical(yw_train, num_classes=n_classes)\n",
        "y_test_cat  = to_categorical(yw_test,  num_classes=n_classes)\n",
        "\n",
        "mlp = build_mlp(n_feats, n_classes)\n",
        "es = callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
        "mlp.fit(Xf_train_s, y_train_cat, validation_split=0.15, epochs=60, batch_size=128, callbacks=[es], verbose=2)\n",
        "mlp_pred = np.argmax(mlp.predict(Xf_test_s), axis=1)\n",
        "print(\"MLP Test accuracy:\", accuracy_score(yw_test, mlp_pred))\n",
        "print(classification_report(yw_test, mlp_pred, target_names=le.classes_))\n",
        "mlp.save(os.path.join(MODEL_DIR, \"mlp_feats.h5\"))\n"
      ],
      "metadata": {
        "id": "8Pz3BNq9Gzyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Soft-vote ensemble (XGB + MLP)\n",
        "xgb_proba = xgb.predict_proba(Xf_test_s)\n",
        "mlp_proba = mlp.predict(Xf_test_s)\n",
        "ens_proba = (xgb_proba + mlp_proba) / 2.0\n",
        "ens_pred = np.argmax(ens_proba, axis=1)\n",
        "print(\"Ensemble (XGB+MLP) Test accuracy:\", accuracy_score(yw_test, ens_pred))\n",
        "print(classification_report(yw_test, ens_pred, target_names=le.classes_))\n",
        "\n",
        "# Save ensemble artifacts\n",
        "joblib.dump(le, os.path.join(MODEL_DIR, \"label_encoder.pkl\"))\n",
        "print(\"Saved label encoder:\", os.path.join(MODEL_DIR, \"label_encoder.pkl\"))\n"
      ],
      "metadata": {
        "id": "LKqOYXHrHSpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Inference helper (use saved models)\n",
        "def time_feats_single(window):\n",
        "    return time_feats(window)\n",
        "\n",
        "def freq_feats_single(window):\n",
        "    return freq_feats(window, n_bands=6)\n",
        "\n",
        "def predict_from_raw_row_no_cnn(raw_row):\n",
        "    \"\"\"raw_row: 1D np.array of sensor values in order sensor_cols\"\"\"\n",
        "    w = np.tile(np.array(raw_row, dtype=float).reshape(1,-1), (TIMESTEPS,1))\n",
        "    feats = np.concatenate([time_feats_single(w), freq_feats_single(w)])\n",
        "    feats_s = scaler_feats.transform(feats.reshape(1,-1))\n",
        "    p_xgb = xgb.predict_proba(feats_s)[0]\n",
        "    p_mlp = mlp.predict(feats_s)[0]\n",
        "    p_avg = (p_xgb + p_mlp)/2.0\n",
        "    idx = np.argmax(p_avg)\n",
        "    return le.inverse_transform([idx])[0], float(p_avg[idx])\n"
      ],
      "metadata": {
        "id": "iOzTl0gzHStF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict_from_raw_row_safe(raw_row):\n",
        "    \"\"\"raw_row: [Vibration_X, Vibration_Y, Vibration_Z, Acoustic_Level, Temperature]\"\"\"\n",
        "    # 1. build artificial window (same as before)\n",
        "    w = np.tile(np.array(raw_row, dtype=float).reshape(1, -1), (TIMESTEPS, 1))\n",
        "\n",
        "    # 2. compute time + freq features using your original functions\n",
        "    t_feats = time_feats(w)\n",
        "    f_feats = freq_feats(w, n_bands=6)\n",
        "    feats = np.concatenate([t_feats, f_feats])\n",
        "\n",
        "    # 3. replace NaN / inf with 0 before scaling\n",
        "    feats = np.nan_to_num(feats, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    # 4. scale and predict (same as before)\n",
        "    feats_s = scaler_feats.transform(feats.reshape(1, -1))\n",
        "    p_xgb = xgb.predict_proba(feats_s)[0]\n",
        "    p_mlp = mlp.predict(feats_s)[0]\n",
        "    p_avg = (p_xgb + p_mlp) / 2.0\n",
        "\n",
        "    idx = int(np.argmax(p_avg))\n",
        "    label = le.inverse_transform([idx])[0]\n",
        "    prob = float(p_avg[idx])\n",
        "\n",
        "    return label, prob\n"
      ],
      "metadata": {
        "id": "HwOBoMhRDoTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Quick sample prediction + list saved artifacts\n",
        "sample_row = df.loc[100, sensor_cols].values\n",
        "plabel, pprob = predict_from_raw_row_no_cnn(sample_row)\n",
        "print(\"Sample prediction:\", plabel, pprob)\n",
        "print(\"Saved files:\", os.listdir(MODEL_DIR))\n"
      ],
      "metadata": {
        "id": "p3cpEqluHSwW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: Optional - XGBoost feature importance (requires matplotlib)\n",
        "importances = xgb.feature_importances_\n",
        "idx = np.argsort(importances)[::-1][:30]\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.barh(range(len(idx)), importances[idx][::-1])\n",
        "plt.yticks(range(len(idx)), [f\"feat_{i}\" for i in idx[::-1]])\n",
        "plt.title(\"Top feature importances (XGBoost)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ZXRmrZujHSyK",
        "outputId": "ce9f79a3-89b6-4e38-c305-77969fc907b3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'xgb' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3851989417.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Cell 14: Optional - XGBoost feature importance (requires matplotlib)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimportances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimportances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'xgb' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_custom(vx, vy, vz, acoustic, temp):\n",
        "    raw = [vx, vy, vz, acoustic, temp]\n",
        "    label, prob = predict_from_raw_row_safe(raw)\n",
        "    print(f\"Input: Vx={vx}, Vy={vy}, Vz={vz}, Acoustic={acoustic}, Temp={temp}\")\n",
        "    print(f\" -> Fault: {label}, Confidence: {prob*100:.2f}%\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "test_custom(0.1, 0.2, 0.3, 88, 90)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "Fa95Q7hB3dn2",
        "outputId": "c229fd06-c603-44ba-b454-870d8a66d37c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'predict_from_raw_row_safe' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4093739907.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtest_custom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m88\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4093739907.py\u001b[0m in \u001b[0;36mtest_custom\u001b[0;34m(vx, vy, vz, acoustic, temp)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_custom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macoustic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macoustic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_from_raw_row_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Input: Vx={vx}, Vy={vy}, Vz={vz}, Acoustic={acoustic}, Temp={temp}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" -> Fault: {label}, Confidence: {prob*100:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'predict_from_raw_row_safe' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fl8mgfJE2ybV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}